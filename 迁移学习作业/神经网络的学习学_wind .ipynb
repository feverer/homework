{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30596 29404\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameters\n",
    "image_size = 28  #图像的总尺寸28*28\n",
    "num_classes = 10  #标签的种类数\n",
    "num_epochs = 100  #训练的总循环周期\n",
    "batch_size = 64  #一个撮（批次）的大小，64张图片\n",
    "\n",
    "#加载MINST数据，如果没有下载过，就会在当前路径下新建/data1子目录，并把文件存放其中\n",
    "train_dataset = dsets.MNIST(root='./data1',\n",
    "                            train=True,\n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "# 加载测试数据集\n",
    "test_dataset = dsets.MNIST(root='./data1',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "\n",
    "# 分割数据集\n",
    "A = [0, 1, 2, 3, 4]\n",
    "num_images = 60000\n",
    "train_A = []\n",
    "train_B = []\n",
    "for i in range(num_images):\n",
    "    if train_dataset[i][1] in A:\n",
    "        train_A.append(train_dataset[i])\n",
    "    else:\n",
    "        train_B.append(train_dataset[i])\n",
    "print(len(train_A),len(train_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练数据集的加载器，按不同训练方式共三个\n",
    "train_loaderA = torch.utils.data.DataLoader(dataset=train_A,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True) \n",
    "\n",
    "train_loaderB = torch.utils.data.DataLoader(dataset=train_B,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "\n",
    "train_loader  = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 校验集和测试集\n",
    "indices = range(len(test_dataset))\n",
    "indices_val = indices[:5000]\n",
    "indices_test = indices[5000:]\n",
    "\n",
    "# 采样器\n",
    "sampler_val = torch.utils.data.sampler.SubsetRandomSampler(indices_val)\n",
    "sampler_test = torch.utils.data.sampler.SubsetRandomSampler(indices_test)\n",
    "\n",
    "# 加载器\n",
    "validation_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                                batch_size = batch_size,\n",
    "                                                shuffle = False,\n",
    "                                                sampler = sampler_val\n",
    "                                               )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          sampler=sampler_test)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义卷积神经网络：4和8为人为指定的两个卷积层的厚度（feature map的数量）\n",
    "depth = [4, 8]\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 该函数在创建一个ConvNet对象的时候，即调用如下语句：net=ConvNet()，就会被调用\n",
    "        # 首先调用父类相应的构造函数\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # 其次构造ConvNet需要用到的各个神经模块。\n",
    "        self.conv1 = nn.Conv2d(1, 4, 5, padding = 2) #定义一个卷积层，输入通道为1，输出通道为4，窗口大小为5，padding为2\n",
    "        self.pool = nn.MaxPool2d(2, 2) #定义一个Pooling层，一个窗口为2*2的pooling运算\n",
    "        self.conv2 = nn.Conv2d(depth[0], depth[1], 5, padding = 2) #第二层卷积，输入通道为depth[0], \n",
    "                                                                   #输出通道为depth[1]，窗口为5，padding为2\n",
    "        self.fc1 = nn.Linear(image_size // 4 * image_size // 4 * depth[1] , 512) \n",
    "                                                            #一个线性连接层，输入尺寸为最后一层立方体的平铺，输出层512个节点\n",
    "        self.fc2 = nn.Linear(512, num_classes) #最后一层线性分类单元，输入为512，输出为要做分类的类别数\n",
    "\n",
    "    def forward(self, x):\n",
    "        #该函数完成神经网络真正的前向运算，我们会在这里把各个组件进行实际的拼装\n",
    "        x = F.relu(self.conv1(x))  #第一层卷积，激活函数用ReLu，为了防止过拟合\n",
    "        x = self.pool(x) #第二层pooling，将图片变小\n",
    "        x = F.relu(self.conv2(x)) #第三层又是卷积，窗口为5，输入输出通道分别为depth[0]=4, depth[1]=8\n",
    "        x = self.pool(x) #第四层pooling，将图片缩小到原大小的1/4\n",
    "        \n",
    "        #将立体的特征图压成一个一维向量\n",
    "        x = x.view(-1, image_size//4 * image_size //4 * depth[1])\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        #为防止过拟合，以默认为0.5的概率对这一层进行dropout操作\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x)\n",
    "        return x\n",
    "                \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 计算准确率的函数\n",
    "def rightness(predictions, labels):\n",
    "    predictions = Variable(predictions)\n",
    "    pred = torch.max(predictions.data, 1)[1]\n",
    "    rights = pred.eq(labels.data.view_as(pred)).sum()\n",
    "    return rights, len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第0周期，第(0/479)个撮，训练误差:2.30,校验误差：2.31, 准确率:0.12\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第1周期，第(0/479)个撮，训练误差:2.26,校验误差：2.30, 准确率:0.12\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第2周期，第(0/479)个撮，训练误差:2.21,校验误差：2.31, 准确率:0.15\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第3周期，第(0/479)个撮，训练误差:2.11,校验误差：2.31, 准确率:0.20\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第4周期，第(0/479)个撮，训练误差:1.95,校验误差：2.38, 准确率:0.15\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第5周期，第(0/479)个撮，训练误差:1.63,校验误差：2.74, 准确率:0.24\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第6周期，第(0/479)个撮，训练误差:1.58,校验误差：3.38, 准确率:0.30\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第7周期，第(0/479)个撮，训练误差:1.38,校验误差：3.84, 准确率:0.42\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第8周期，第(0/479)个撮，训练误差:1.10,校验误差：4.18, 准确率:0.40\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第9周期，第(0/479)个撮，训练误差:0.79,校验误差：4.61, 准确率:0.41\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第10周期，第(0/479)个撮，训练误差:0.59,校验误差：4.96, 准确率:0.43\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第11周期，第(0/479)个撮，训练误差:0.52,校验误差：5.30, 准确率:0.44\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第12周期，第(0/479)个撮，训练误差:0.48,校验误差：5.45, 准确率:0.47\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第13周期，第(0/479)个撮，训练误差:0.28,校验误差：5.63, 准确率:0.48\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第14周期，第(0/479)个撮，训练误差:0.33,校验误差：5.74, 准确率:0.47\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第15周期，第(0/479)个撮，训练误差:0.21,校验误差：5.79, 准确率:0.48\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第16周期，第(0/479)个撮，训练误差:0.48,校验误差：5.89, 准确率:0.48\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第17周期，第(0/479)个撮，训练误差:0.17,校验误差：5.92, 准确率:0.49\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第18周期，第(0/479)个撮，训练误差:0.20,校验误差：5.95, 准确率:0.49\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第19周期，第(0/479)个撮，训练误差:0.12,校验误差：5.99, 准确率:0.49\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第20周期，第(0/479)个撮，训练误差:0.24,校验误差：5.99, 准确率:0.49\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第21周期，第(0/479)个撮，训练误差:0.29,校验误差：5.89, 准确率:0.49\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第22周期，第(0/479)个撮，训练误差:0.14,校验误差：5.83, 准确率:0.49\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第23周期，第(0/479)个撮，训练误差:0.09,校验误差：5.92, 准确率:0.49\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第24周期，第(0/479)个撮，训练误差:0.27,校验误差：5.83, 准确率:0.49\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第25周期，第(0/479)个撮，训练误差:0.19,校验误差：5.77, 准确率:0.49\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第26周期，第(0/479)个撮，训练误差:0.10,校验误差：5.66, 准确率:0.49\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第27周期，第(0/479)个撮，训练误差:0.24,校验误差：5.58, 准确率:0.49\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第28周期，第(0/479)个撮，训练误差:0.30,校验误差：5.69, 准确率:0.49\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第29周期，第(0/479)个撮，训练误差:0.21,校验误差：5.68, 准确率:0.49\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第30周期，第(0/479)个撮，训练误差:0.30,校验误差：5.39, 准确率:0.49\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第31周期，第(0/479)个撮，训练误差:0.13,校验误差：5.47, 准确率:0.49\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第32周期，第(0/479)个撮，训练误差:0.24,校验误差：5.47, 准确率:0.49\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第33周期，第(0/479)个撮，训练误差:0.13,校验误差：5.37, 准确率:0.50\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第34周期，第(0/479)个撮，训练误差:0.12,校验误差：5.30, 准确率:0.50\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第35周期，第(0/479)个撮，训练误差:0.24,校验误差：5.41, 准确率:0.50\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第36周期，第(0/479)个撮，训练误差:0.10,校验误差：5.34, 准确率:0.50\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第37周期，第(0/479)个撮，训练误差:0.13,校验误差：5.22, 准确率:0.50\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第38周期，第(0/479)个撮，训练误差:0.23,校验误差：5.26, 准确率:0.50\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第39周期，第(0/479)个撮，训练误差:0.12,校验误差：5.37, 准确率:0.50\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第40周期，第(0/479)个撮，训练误差:0.11,校验误差：5.37, 准确率:0.50\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第41周期，第(0/479)个撮，训练误差:0.29,校验误差：5.30, 准确率:0.50\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第42周期，第(0/479)个撮，训练误差:0.07,校验误差：5.29, 准确率:0.50\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第43周期，第(0/479)个撮，训练误差:0.30,校验误差：5.43, 准确率:0.50\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第44周期，第(0/479)个撮，训练误差:0.18,校验误差：5.28, 准确率:0.50\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第45周期，第(0/479)个撮，训练误差:0.03,校验误差：5.34, 准确率:0.49\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第46周期，第(0/479)个撮，训练误差:0.08,校验误差：5.33, 准确率:0.50\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第47周期，第(0/479)个撮，训练误差:0.08,校验误差：5.36, 准确率:0.50\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第48周期，第(0/479)个撮，训练误差:0.06,校验误差：5.35, 准确率:0.50\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例20，第49周期，第(0/479)个撮，训练误差:0.06,校验误差：5.36, 准确率:0.50\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第50周期，第(0/460)个撮，训练误差:10.60,校验误差：4.85, 准确率:0.50\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第51周期，第(0/460)个撮，训练误差:1.48,校验误差：3.00, 准确率:0.22\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第52周期，第(0/460)个撮，训练误差:1.19,校验误差：3.66, 准确率:0.31\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第53周期，第(0/460)个撮，训练误差:0.90,校验误差：3.70, 准确率:0.36\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第54周期，第(0/460)个撮，训练误差:0.64,校验误差：3.80, 准确率:0.40\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第55周期，第(0/460)个撮，训练误差:0.65,校验误差：3.95, 准确率:0.41\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第56周期，第(0/460)个撮，训练误差:0.37,校验误差：4.06, 准确率:0.41\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第57周期，第(0/460)个撮，训练误差:0.49,校验误差：4.06, 准确率:0.42\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第58周期，第(0/460)个撮，训练误差:0.26,校验误差：4.13, 准确率:0.42\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第59周期，第(0/460)个撮，训练误差:0.47,校验误差：4.16, 准确率:0.43\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第60周期，第(0/460)个撮，训练误差:0.43,校验误差：4.17, 准确率:0.43\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第61周期，第(0/460)个撮，训练误差:0.26,校验误差：4.29, 准确率:0.43\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第62周期，第(0/460)个撮，训练误差:0.40,校验误差：4.25, 准确率:0.43\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第63周期，第(0/460)个撮，训练误差:0.53,校验误差：4.27, 准确率:0.43\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第64周期，第(0/460)个撮，训练误差:0.31,校验误差：4.28, 准确率:0.44\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第65周期，第(0/460)个撮，训练误差:0.23,校验误差：4.31, 准确率:0.43\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第66周期，第(0/460)个撮，训练误差:0.30,校验误差：4.36, 准确率:0.44\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第67周期，第(0/460)个撮，训练误差:0.31,校验误差：4.13, 准确率:0.44\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第68周期，第(0/460)个撮，训练误差:0.22,校验误差：4.18, 准确率:0.44\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第69周期，第(0/460)个撮，训练误差:0.38,校验误差：4.25, 准确率:0.44\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第70周期，第(0/460)个撮，训练误差:0.21,校验误差：4.17, 准确率:0.44\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第71周期，第(0/460)个撮，训练误差:0.39,校验误差：4.34, 准确率:0.44\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第72周期，第(0/460)个撮，训练误差:0.19,校验误差：4.23, 准确率:0.45\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第73周期，第(0/460)个撮，训练误差:0.17,校验误差：4.20, 准确率:0.44\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第74周期，第(0/460)个撮，训练误差:0.18,校验误差：4.27, 准确率:0.45\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第75周期，第(0/460)个撮，训练误差:0.22,校验误差：4.31, 准确率:0.44\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第76周期，第(0/460)个撮，训练误差:0.21,校验误差：4.20, 准确率:0.45\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第77周期，第(0/460)个撮，训练误差:0.25,校验误差：4.34, 准确率:0.45\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第78周期，第(0/460)个撮，训练误差:0.19,校验误差：4.42, 准确率:0.45\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第79周期，第(0/460)个撮，训练误差:0.12,校验误差：4.19, 准确率:0.45\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第80周期，第(0/460)个撮，训练误差:0.12,校验误差：4.35, 准确率:0.45\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第81周期，第(0/460)个撮，训练误差:0.10,校验误差：4.35, 准确率:0.45\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第82周期，第(0/460)个撮，训练误差:0.15,校验误差：4.20, 准确率:0.45\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第83周期，第(0/460)个撮，训练误差:0.23,校验误差：4.40, 准确率:0.45\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第84周期，第(0/460)个撮，训练误差:0.25,校验误差：4.41, 准确率:0.45\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第85周期，第(0/460)个撮，训练误差:0.29,校验误差：4.36, 准确率:0.45\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第86周期，第(0/460)个撮，训练误差:0.19,校验误差：4.18, 准确率:0.46\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第87周期，第(0/460)个撮，训练误差:0.27,校验误差：4.20, 准确率:0.46\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第88周期，第(0/460)个撮，训练误差:0.23,校验误差：4.06, 准确率:0.46\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第89周期，第(0/460)个撮，训练误差:0.24,校验误差：4.04, 准确率:0.46\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第90周期，第(0/460)个撮，训练误差:0.18,校验误差：4.32, 准确率:0.45\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第91周期，第(0/460)个撮，训练误差:0.34,校验误差：4.43, 准确率:0.46\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第92周期，第(0/460)个撮，训练误差:0.14,校验误差：4.11, 准确率:0.46\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第93周期，第(0/460)个撮，训练误差:0.17,校验误差：4.37, 准确率:0.45\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第94周期，第(0/460)个撮，训练误差:0.15,校验误差：4.16, 准确率:0.46\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第95周期，第(0/460)个撮，训练误差:0.14,校验误差：4.14, 准确率:0.46\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第96周期，第(0/460)个撮，训练误差:0.11,校验误差：4.31, 准确率:0.46\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第97周期，第(0/460)个撮，训练误差:0.14,校验误差：4.22, 准确率:0.46\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第98周期，第(0/460)个撮，训练误差:0.16,校验误差：4.02, 准确率:0.46\n",
      "0~4/5~9网络：第2阶段,第0次试验，数据比例20，第99周期，第(0/460)个撮，训练误差:0.13,校验误差：4.11, 准确率:0.46\n",
      "0~4/5~9 0 20\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例10，第0周期，第(0/479)个撮，训练误差:2.30,校验误差：2.30, 准确率:0.10\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例10，第1周期，第(0/479)个撮，训练误差:2.22,校验误差：2.30, 准确率:0.09\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例10，第2周期，第(0/479)个撮，训练误差:2.02,校验误差：2.33, 准确率:0.10\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例10，第3周期，第(0/479)个撮，训练误差:1.55,校验误差：3.05, 准确率:0.19\n",
      "0~4/5~9网络：第1阶段,第0次试验，数据比例10，第4周期，第(0/479)个撮，训练误差:1.18,校验误差：4.08, 准确率:0.45\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-99b52640010e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#清空梯度\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-b2fa5b0d8c95>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#该函数完成神经网络真正的前向运算，我们会在这里把各个组件进行实际的拼装\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#第一层卷积，激活函数用ReLu，为了防止过拟合\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#第二层pooling，将图片变小\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#第三层又是卷积，窗口为5，输入输出通道分别为depth[0]=4, depth[1]=8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 254\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     50\u001b[0m     f = ConvNd(_pair(stride), _pair(padding), _pair(dilation), False,\n\u001b[1;32m     51\u001b[0m                _pair(0), groups, torch.backends.cudnn.benchmark, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = {}  #试验结果\n",
    "times = 10 \n",
    "fractions = [20, 10, 8, 6, 5, 4, 3, 2, 1] #数据加载比例值\n",
    "\n",
    "num_epoch1 = 50\n",
    "num_epoch2 = 100\n",
    "\n",
    "for experiment in ['0~4/5~9','0~4/0~9', '0~9']:  \n",
    "    for time in range(times):\n",
    "        for fraction in fractions:\n",
    "            \n",
    "            \n",
    "            if experiment == '0~4/5~9':\n",
    "                net = ConvNet()\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "                trainData = train_loaderA\n",
    "            if experiment =='0~4/0~9':\n",
    "                trainData = train_loaderA\n",
    "                net = ConvNet()\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "            if experiment == '0~9':\n",
    "                trainData = train_loader\n",
    "                net = ConvNet()\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "                \n",
    "            # 开始训练\n",
    "            records = []\n",
    "            for epoch in range(num_epoch1):\n",
    "                losses = []\n",
    "                for idx, (data, label) in enumerate(trainData):\n",
    "                    if idx >= (len(trainData) // fraction):\n",
    "                        break\n",
    "                    data, label = Variable(data), Variable(label)\n",
    "                    net.train()\n",
    "                    optimizer.zero_grad() #清空梯度\n",
    "                    \n",
    "                    output = net(data)\n",
    "                    loss = criterion(output,label)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    losses.append(loss.data.numpy())\n",
    "                    \n",
    "                    if idx % 200 == 0:\n",
    "                        val_losses = []\n",
    "                        rights =[]\n",
    "                        net.eval()\n",
    "                        for val_data in validation_loader:\n",
    "                            (data, label) = val_data\n",
    "                            data, label = Variable(data), Variable(label)\n",
    "                            output = net(data)\n",
    "                            loss = criterion(output, label)\n",
    "                            val_losses.append(loss.data.numpy())\n",
    "                            right = rightness(output.data, label)\n",
    "                            rights.append(right)\n",
    "                        right_ratio = 1.0 * np.sum([i[0] for i in rights]) / sum([i[1] for i in rights])\n",
    "                        records.append([np.mean(losses), np.mean(val_losses), right_ratio])\n",
    "                        print('{}网络：第1阶段,第{}次试验，数据比例{}，第{}周期，第({}/{})个撮，训练误差:{:.2f},校验误差：{:.2f}, 准确率:{:.2f}'\n",
    "                              .format(experiment, time, fraction, epoch, idx, len(trainData),\n",
    "                              np.mean(losses), np.mean(val_losses), right_ratio))\n",
    "                \n",
    "            for epoch in range(num_epoch1, num_epoch2):\n",
    "                if experiment == '0~4/5~9':\n",
    "                        trainData = train_loaderB\n",
    "                if experiment == '0~4/0~9':\n",
    "                        trainData = train_loader\n",
    "                if experiment == '0~9':\n",
    "                        trainData = train_loader\n",
    "                    \n",
    "                losses = []\n",
    "                for idx, (data, label) in enumerate(trainData):\n",
    "                    if idx >= (len(trainData) // fraction):\n",
    "                        break  \n",
    "                    \n",
    "                    net.train()\n",
    "                    optimizer.zero_grad() #清空梯度\n",
    "                    data, label = Variable(data), Variable(label)\n",
    "                    output = net(data)\n",
    "                    loss = criterion(output,label)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    losses.append(loss.data.numpy())\n",
    "                    \n",
    "                    if idx % 200 == 0:\n",
    "                        val_losses = []\n",
    "                        rights =[]\n",
    "                        net.eval()\n",
    "                        for val_data in validation_loader:\n",
    "                            (data, label) = val_data\n",
    "                            data, label = Variable(data), Variable(label)\n",
    "                            output = net(data)\n",
    "                            loss = criterion(output, label)\n",
    "                            val_losses.append(loss.data.numpy())\n",
    "                            right = rightness(output.data, label)\n",
    "                            rights.append(right)\n",
    "                        right_ratio = 1.0 * np.sum([i[0] for i in rights]) / sum([i[1] for i in rights])\n",
    "                        records.append([np.mean(losses), np.mean(val_losses), right_ratio])\n",
    "                        print('{}网络：第2阶段,第{}次试验，数据比例{}，第{}周期，第({}/{})个撮，训练误差:{:.2f},校验误差：{:.2f}, 准确率:{:.2f}'\n",
    "                              .format(experiment, time, fraction, epoch, idx, len(trainData),\n",
    "                              np.mean(losses), np.mean(val_losses), right_ratio))\n",
    "                                 \n",
    "            test_rights = []\n",
    "            net.eval()\n",
    "            for test_data in test_loader:\n",
    "                (data, label) = test_data\n",
    "                data, label = Variable(data), Variable(label)\n",
    "                output = net(data)\n",
    "                loss = criterion(output, label)\n",
    "                right = rightness(output.data, label)\n",
    "                test_rights.append(right)\n",
    "            right_ratio = 1.0 * np.sum([i[0] for i in test_rights]) / np.sum([i[1] for i in test_rights])\n",
    "            print(experiment, time, fraction)\n",
    "                    \n",
    "            results[(experiment, time, fraction)] = [records, right_ratio]\n",
    "                            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_curve = {}\n",
    "tests = {}\n",
    "for experiment in ['0~4/5~9','0~4/0~9', '0~9']:\n",
    "    for fraction in fractions:\n",
    "        one_experiment = []\n",
    "        test_value = []\n",
    "        for time in range(times):\n",
    "            rr = results[(experiment, time, fraction)]\n",
    "            one_experiment.append([ii[2] for ii in rr[0]])\n",
    "            test_value.append(rr[1])\n",
    "        aa = np.array(one_experiment)\n",
    "        #print(aa.shape)\n",
    "        one_curve1[(experiment, fraction)] = np.mean(aa, 0)\n",
    "        tests1[(experiment, fraction)] = np.mean(test_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 然后再绘制它们的误差曲线\n",
    "for fraction in fractions:\n",
    "    plt.figure(figsize = (10, 7))\n",
    "    plt.title('{:.2f} % of the data'.format(100.0 / fraction))\n",
    "    plt.plot(1 - one_curve1[('0~4/5~9', fraction)], label = '0~9')\n",
    "    plt.plot(1 - one_curve1[('0~4/0~9', fraction)], label = '0~4/0~9')\n",
    "    plt.plot(1 - one_curve1[('0~9', fraction)], label = '0~9')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制测试准确度随着fraction变化的曲线\n",
    "plt.figure(figsize = (10, 7))\n",
    "for experiment in ['0~4/5~9']:\n",
    "    testss = []\n",
    "    for fraction in fractions:\n",
    "        test = 1 - tests[(experiment, fraction)]\n",
    "        testss.append(test)\n",
    "    plt.plot(fractions, testss, 'o-', label = experiment)\n",
    "plt.legend()\n",
    "plt.xlabel('Fractions')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制测试准确度随着fraction变化的曲线\n",
    "plt.figure(figsize = (10, 7))\n",
    "for experiment in ['0~4/5~9','0~4/0~9', '0~9']:\n",
    "    testss = []\n",
    "    for fraction in fractions:\n",
    "        test = 1 - tests1[(experiment, fraction)]\n",
    "        testss.append(test)\n",
    "    plt.plot(fractions, testss, 'o-', label = experiment)\n",
    "plt.legend()\n",
    "plt.xlabel('Fractions')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
